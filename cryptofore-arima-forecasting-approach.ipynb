{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":30894,"databundleVersionId":2815541,"sourceType":"competition"}],"dockerImageVersionId":30145,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/cryptofore-arima-forecasting-approach?scriptVersionId=216324944\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Welcome to our Universe\n## [@donutz.ai](www.donutz.ai/#)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom IPython.display import clear_output\nclear_output()\nimport time,csv\nimport matplotlib.pyplot as plt\nfrom dateutil.tz import tzlocal\nfrom datetime import datetime\nimport seaborn as sns\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-21T21:44:49.40393Z","iopub.execute_input":"2021-11-21T21:44:49.404208Z","iopub.status.idle":"2021-11-21T21:44:49.657108Z","shell.execute_reply.started":"2021-11-21T21:44:49.404182Z","shell.execute_reply":"2021-11-21T21:44:49.656262Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        clear_output()\n\n# could've done this better, but hey...\n# for kaggle\n# rootdir = '/kaggle/input/g-research-crypto-forecasting/'\n\n# for home\nrootdir = '/kaggle/input/g-research-crypto-forecasting/'\n\n# data pull\nexample_sample_submission = pd.read_csv(rootdir+'example_sample_submission.csv')\nasset_details = pd.read_csv(rootdir+'asset_details.csv')\nexample_test = pd.read_csv(rootdir+'example_test.csv')\ntrain = pd.read_csv(rootdir+'train.csv')\nsupplemental_train = pd.read_csv(rootdir+'supplemental_train.csv')\n\nprint('csv/files loaded')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:45:33.675693Z","iopub.execute_input":"2021-11-21T21:45:33.676485Z","iopub.status.idle":"2021-11-21T21:46:41.562604Z","shell.execute_reply.started":"2021-11-21T21:45:33.676448Z","shell.execute_reply":"2021-11-21T21:46:41.561554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# timestamping \nsupplemental_train['timestamp'] = supplemental_train.timestamp.astype('datetime64[s]')\nexample_test['timestamp'] = example_test.timestamp.astype('datetime64[s]')\ntrain['timestamp'] = train.timestamp.astype('datetime64[s]')\nprint('timestamping complete')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:41.565492Z","iopub.execute_input":"2021-11-21T21:46:41.565884Z","iopub.status.idle":"2021-11-21T21:46:43.885021Z","shell.execute_reply.started":"2021-11-21T21:46:41.565825Z","shell.execute_reply":"2021-11-21T21:46:43.884103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy\nimport matplotlib.pyplot as plt\nimport math\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:43.886636Z","iopub.execute_input":"2021-11-21T21:46:43.886985Z","iopub.status.idle":"2021-11-21T21:46:50.140881Z","shell.execute_reply.started":"2021-11-21T21:46:43.886939Z","shell.execute_reply":"2021-11-21T21:46:50.14021Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n# fix random seed for reproducibility\nnumpy.random.seed(7)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:50.142528Z","iopub.execute_input":"2021-11-21T21:46:50.143188Z","iopub.status.idle":"2021-11-21T21:46:50.151903Z","shell.execute_reply.started":"2021-11-21T21:46:50.143153Z","shell.execute_reply":"2021-11-21T21:46:50.15013Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the dataset\n# isolate to only one asset, for now...\n# this also assumes only one variable - & we do not expect to output a complete \"submission.csv\"\ndataframe = supplemental_train[supplemental_train.Asset_ID == 8][['Close']].reset_index(drop=True).head(100)\n\ndataset = dataframe.values\ndataset = dataset.astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:50.152933Z","iopub.execute_input":"2021-11-21T21:46:50.154109Z","iopub.status.idle":"2021-11-21T21:46:50.218211Z","shell.execute_reply.started":"2021-11-21T21:46:50.153959Z","shell.execute_reply":"2021-11-21T21:46:50.217376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:50.219834Z","iopub.execute_input":"2021-11-21T21:46:50.220312Z","iopub.status.idle":"2021-11-21T21:46:50.226643Z","shell.execute_reply.started":"2021-11-21T21:46:50.220266Z","shell.execute_reply":"2021-11-21T21:46:50.225786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n\n# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:50.228073Z","iopub.execute_input":"2021-11-21T21:46:50.228304Z","iopub.status.idle":"2021-11-21T21:46:50.246556Z","shell.execute_reply.started":"2021-11-21T21:46:50.228277Z","shell.execute_reply":"2021-11-21T21:46:50.245716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # ARIMA Model","metadata":{}},{"cell_type":"code","source":"# Influence: https://machinelearningmastery.com/grid-search-arima-hyperparameters-with-python/","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:43:30.328762Z","iopub.execute_input":"2021-11-21T21:43:30.329235Z","iopub.status.idle":"2021-11-21T21:43:30.33218Z","shell.execute_reply.started":"2021-11-21T21:43:30.329203Z","shell.execute_reply":"2021-11-21T21:43:30.331636Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.tsa.arima.model import ARIMA\nfrom matplotlib import pyplot\n# load dataset\n# dataframe\n\n# fit model\nmodel = ARIMA(dataframe, order=(5,1,0))\nmodel_fit = model.fit()\n# summary of fit model\nprint(model_fit.summary())\n# line plot of residuals\nresiduals = pd.DataFrame(model_fit.resid)\nresiduals.plot()\npyplot.show()\n# density plot of residuals\nresiduals.plot(kind='kde')\npyplot.show()\n# summary stats of residuals\nprint(residuals.describe())","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:53.569288Z","iopub.execute_input":"2021-11-21T21:46:53.569759Z","iopub.status.idle":"2021-11-21T21:46:54.361398Z","shell.execute_reply.started":"2021-11-21T21:46:53.569709Z","shell.execute_reply":"2021-11-21T21:46:54.36068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grid search ARIMA parameters for time series\nimport warnings\nfrom math import sqrt\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(X, arima_order):\n\t# prepare training dataset\n\ttrain_size = int(len(X) * 0.66)\n\ttrain, test = X[0:train_size], X[train_size:]\n\thistory = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(len(test)):\n\t\tmodel = ARIMA(history, order=arima_order)\n\t\tmodel_fit = model.fit()\n\t\tyhat = model_fit.forecast()[0]\n\t\tpredictions.append(yhat)\n\t\thistory.append(test[t])\n\t# calculate out of sample error\n\trmse = sqrt(mean_squared_error(test, predictions))\n\treturn rmse\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n\tdataset = dataset.astype('float32')\n\tbest_score, best_cfg = float(\"inf\"), None\n\tfor p in p_values:\n\t\tfor d in d_values:\n\t\t\tfor q in q_values:\n\t\t\t\torder = (p,d,q)\n\t\t\t\ttry:\n\t\t\t\t\trmse = evaluate_arima_model(dataset, order)\n\t\t\t\t\tif rmse < best_score:\n\t\t\t\t\t\tbest_score, best_cfg = rmse, order\n\t\t\t\t\tprint('ARIMA%s RMSE=%.3f' % (order,rmse))\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\tprint('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n\n# load dataset\nseries = dataframe.Close\n\n# evaluate parameters\np_values = [0,1,2] # please use -> [0, 1, 2, 4, 6, 8, 10]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(series.values, p_values, d_values, q_values)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:46:54.682937Z","iopub.execute_input":"2021-11-21T21:46:54.683235Z","iopub.status.idle":"2021-11-21T21:48:48.037137Z","shell.execute_reply.started":"2021-11-21T21:46:54.683202Z","shell.execute_reply":"2021-11-21T21:48:48.034755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nmodel = SARIMAX(dataframe.Close, exog=None, order=(0, 1, 1), seasonal_order=(0, 1, 1, 7)).fit(method='cg')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:48:48.039289Z","iopub.execute_input":"2021-11-21T21:48:48.039617Z","iopub.status.idle":"2021-11-21T21:48:49.609832Z","shell.execute_reply.started":"2021-11-21T21:48:48.039574Z","shell.execute_reply":"2021-11-21T21:48:49.602162Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"help(model)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"#import gresearch_crypto\n#env = gresearch_crypto.make_env()\n\n# Training data is in the competition dataset as usual\n#train_df = pd.read_csv('/kaggle/input/g-research-crypto-forecasting/train.csv', low_memory=False)\n\n# load model\n#model.fit(train_df)\n#model.fit(train_df)\n\n#iter_test = env.iter_test()\n\n#for (test_df, sample_prediction_df) in iter_test:\n#    sample_prediction_df['Target'] = model.predict(test_df)\n#    env.predict(sample_prediction_df)\n\n\n# submission\n#sample_prediction_df['Target'].to_csv('submission.csv', index=False)\nprint(\"project complete, for now.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:48:49.611703Z","iopub.execute_input":"2021-11-21T21:48:49.612153Z","iopub.status.idle":"2021-11-21T21:48:49.619001Z","shell.execute_reply.started":"2021-11-21T21:48:49.612103Z","shell.execute_reply":"2021-11-21T21:48:49.618076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# still need output: submission.csv\n\n# to be continued...","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:48:49.621758Z","iopub.execute_input":"2021-11-21T21:48:49.622186Z","iopub.status.idle":"2021-11-21T21:48:49.632831Z","shell.execute_reply.started":"2021-11-21T21:48:49.62214Z","shell.execute_reply":"2021-11-21T21:48:49.631778Z"},"trusted":true},"outputs":[],"execution_count":null}]}